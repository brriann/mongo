https://docs.mongodb.com/manual/meta/aggregation-quick-reference/

###
### $match - FILTERING DOCUMENTS
###

-$match should come as early as possible, and can be used multiple times in an aggregation pipeline
-$match uses standard MongoDB query operators
  -no $where.  $text only allowed at first stage of aggregation pipeline
-$match in the first stage, can take advantage of INDEXES.  SO, $MATCH = FIRST STAGE!!!
-$match does not allow projection.  
-$match is a FILTER, not a find

db.coll.aggregate([{ $match: {} }])

###
### $project - SHAPING DOCUMENTS
###

-$project is like a find projection, and much more. remove/retain fields, reassign fields, combine into new fields
-$project is like a map function - higher order function that transforms a collection

db.coll.aggregate([{ $project: {...} }])
db.coll.aggregate([{ $project: {_id: 0, name: 1} }]) // REMOVE or RETAIN, just like projection in a find
db.coll.aggregate([{ $project: {_id: 0, name: 1, "field.subfieldOnly": 1} }])

db.coll.aggregate([{ $project: {_id: 0, name: 1, field: "$field.subfieldOnly"} }]) // REASSIGN SUBFIELD TO FIELD NAME (field path expression)

db.coll.aggregate([{ $project: {_id: 0, name: 1, newFieldName: "$field.subfieldOnly"} }]) // CREATE NEW FIELD TO HOUSE THE SUBFIELD

db.coll.aggregate([{ $project: {_id: 0, name: 1, resultOfMath: { $multiply: [ {$divide: ["$field.subfieldOnly", 25]},
                                                                             77 ]}}
}]) // math operators on a field path expression, reassigned to new field

$MAP, $MATCH AND $PROJECT WITH ARRAYS

{ $match: { arrayField: { $elemMatch: { $exists: true } } } // CHECK IF FIELD IS AN ARRAY, AND IS NOT EMPTY

arrayField: { $map: { input: "$arrayField", as: "arrayElt", in: { $arrayElemAt: [ { $split: ["$$arrayElt", "i"] }, 0 ] } } } // map function over each array field element ... split at the char "i" and take the 0th portion


###
### $addFields - SIMILAR TO PROJECT ... MODIFY INCOMING PIPELINE DOCUMENTS WITH NEW COMPUTED FIELDS, OR CHANGE EXISTING FIELDS
###

-$project can be tedious in specifying lots of fields which you want to keep
-$addFields can perform transformations/add new fields, without stripping other non-explicitly retained fields
db.coll.aggregate([
{ $project: {_id: 0, name: 1, gravity: 1, mass: 1} },
{ $addFields: { gravity: "$gravity.value", mass: "$mass.value" } } ]) // RETAIN FIELDS WITH $PROJECT, THEN TRANSFORM WITH $ADDFIELDS

###
### $geoNear - for working with geoJSON data
###

###
### $sort, $skip, $limit, $count - Cursor-like stages
###

Cursor equivalents:

find().count()
find().skip(5)
find().limit(5)
find().sort({someField: -1}) // .sort({firstBy: 1, thenBy: -1})

$limit: {integer}
$skip: {integer}
$count: {<name what the count should be called>} 
$sort: {<fieldToSortOn>: <integer for sort direction>}

-$count counts all incoming documents to the pipeline stage
-$count's count is an END RESULT of an aggregation pipeline

-$sort: {firstBy: -1, thenBy: 1} // CAN SORT BY MULTIPLE FIELDS, JUST AS IN A CURSOR

-$sort, if early in the pipeline, and BEFORE a $project, can take advantage of INDEXES
-otherwise, $sort is in-memory, and greatly increases memory usage.
-pipeline option { allowDiskUse: true } helps us use DISK as well as RAM to perform SORTS

###
### $sample - selects a set of random documents from a collection
###

1st way: { $sample: <size N, # documents> }
-WHEN N <= 5% of total documents #, AND source collection has >=- 100 documents AND $sample is first stage
--a pseudorandom cursor selects N documents to be passed on
-ELSE
--an in-memory random sort happens, and N documents are selected


###
### $group
###

{$group: {_id: <match/group criteria> } }
{$group: {_id: $fieldNameIWantToGroupBy } }

this outputs:

{"_id": 2017}
{"_id": 2018}
{"_id": 2019}
...
if grouping by a year field.

-grouping by just one equivalent is the same as a "distinct" command

{$group: {_id: fieldNameToGroupBy,
  fieldName: <accumulatorExpression1>,
  fieldName: <accumulatorExpression2>,
  ...
} }

{$group: {_id: "$year",
  numFilmsInYear: {$sum: 1}, //each matching document sums +=1 to the numFilmsInYear aggregation field.
  averageReview: {$avg: "$imdbRating"} // $avg average operator
} }

// grouping by a year field. getting the count of films in that year.  and sorting by the count field in another stage

{$group: {_id: "$year",
  numFilmsInYear: {$sum: 1} //each matching document sums +=1 to the numFilmsInYear aggregation field.
} },
{sort: { numFilmsInYear: -1 }}

{ $group: _id: {numOfArrayElts: {$cond: [{$isArray: "$arrayField"}, {$size: "$arrayField"}, 0]}} } // $cond = ternary statement ... either array field size, or 0

{$group: {_id: null}} // GROUP BY NULL OR '' ID GROUPS ALL DOCUMENTS. THERE IS NO MATCH/SUBSET BUCKETING

// use case: get an average value for all documents meeting an intitial $match stage

db.movies.aggregate([
{$match:{rating: {$gte: 0}}},
{$group: {_id: null, averateRaiting: { $avg: "$rating" }}}
])

Takeaways:
_id is essentially GROUP BY ___
$group can accept all accumulator expressions
$group can be used multiple times within a pipeline



###
### Accumulator expressions within $project
###

$min, $max, $avg, $sum, $stdDevPop, $stdDevSam

$reduce and $map -- for more complex calculations

-Accumulator expressions within $project operator over an ***ARRAY FIELD*** WITHIN THE CURRENT DOCUMENT

eg, {_id: 1, dataArray: [1,2,3]}
  {_id: 2, dataArray: [4,5,6]}
  
db.coll.aggregate([{$project: {dataArrayAverage: {$avg: "$dataArray"}}}])
>>
{_id: 1, dataArrayAverage: 2}
{_id: 2, dataArrayAverage: 5} 

$reduce accumulator:
  https://docs.mongodb.com/manual/reference/operator/aggregation/reduce/
  $this: current elt in array
  $$variables: for use only within scope of reduce function

db.coll.aggregate([{$project: {dataArrayMax: {$max: "$dataArray"}}}])


###
### $unwind - create a new document for every entry in an array field, an "un-reduce"
###

{id: "a", values: [1,2,3]}
{id: "b", values: [6,7,8]}

db.coll.aggregate([{$unwind: "values"}])

> {id: a, values: 1}
> {id: a, values: 2}
> {id: a, values: 3}
...etc

use case: grouping by array individual elements

$first operator in a subsequent $group 

short vs long form ...

$unwind: <field path>

$unwind: {path: <field path>, includeArrayIndex: <string>, preserveNullAndEmptyArrays: <boolean>}

-$match EARLY
-retain only needed info with $project
-allowDiskUse if necessary

###
### $lookup - a "left outer join" -- combine documents from 2 different collections
###

-combine ALL documents from collection on left, with MATCHING documents from collection on right

$lookup: {
  from: <collection to join>
  localField: <field from input documents>, //single value or array
  foreignField: <field from documents of "from" collection>, //single value or array
  as: <output array field>
}

can use a $match stage after $lookup to filter out [] empty arrays, with no matches eg.

###
### $graphLookup
###

SQL- recurvise CTE / transitive closure

$graphLookup: looking up recursively a set of documents, with a defined relationship to a starting document.

$graphLookup: {
  from: <lookup table>,
  startWith: <expression for value to start from>,
  connectFromField: <field name to connect from>,
  connectToField: <field name to connect to>,
  as: <field name FOR RESULT ARRAY>,
  maxDepth: <# iterations to perform, optional>,
  depthField: <field name for # iterations to reach this node IN RESULT ARRAY, optional>,
  restrictSearchWithMatch: <match condition to apply to lookup, optional>,
}

// this $graphLookup handles documents with a reference to their BOSS

{_id: 1, name: Dave, reportsTo: null} // CEO
{_id: 2, name: Bob, reportsTo: 2} // CTO

db.orgChart.aggregate([
{$match: {name: "Boss"}},
{$graphLookup: {
  from: "orgChart",
  startWith: "$_id", // from $match'd document
  connectFromField: "_id", //field in parent document
  connectToField: "reportsTo", //field in child document
  as: 'reportsThatReportToBoss' // stuff resulting documents in array
}}
])

maxDepth and depthField -- handles # iterations, etc

restrictSearchWithMatch -- additional $match stage within $graphLookup

GENERAL $GRAPHLOOKUP CONSIDERATIONS
-memory allocation => allowDiskUse
-Indexes => connectToField should be indexed for better $graphLookup performance
-Sharding => FROM COLLECTION cannot be sharded
-$match => $match stages are not optimized, if not related to $graphLookup... (???)


###
### Facets
###

- multiple filters and characterizations, along multiple dimension (facets)

$sortByCount operator : groups by a field, sorts matches by count  (similar to a group by, count(*), order by clause)

MANUAL BUCKETS
-bucketing: grouping values by discrete ranges, instead of individual values

{$bucket: {
  "groupBy": "$numberOfEmployees",
  "boundaries": [0, 20, 50, 500, 1000, Infinity],
  "default": "Other" // OPTIONAL default bucket catches non-Int datatypes, eg String
}}

AUTO BUCKETS

https://docs.mongodb.com/manual/reference/operator/aggregation/bucketAuto/

{$bucketAuto: {
  groupBy: "$aFieldName",
  buckets: 5
}}
Series
Granularity

MULTIPLE FACETS

{$match: {$text: {$search: "mySearchTerm"}}}

$facet operator
https://docs.mongodb.com/manual/reference/operator/aggregation/facet/

-each sub-pipeline within $facet is passed the exact same input documents

$sortByCount

equivalent to :

{$group: {_id: "fieldToGroupBy": {$sum: 1}}},
{$sort: {"count": -1}}


###
### MISC AGGREGATION
###

$redact
https://docs.mongodb.com/manual/reference/operator/aggregation/redact/

{$redact: <expression>}
-expression must resolve to:

$$DESCEND - retains current level and evaluates the next level down 
$$PRUNE - apply to all levels below evaluated level
$$KEEP - apply to all levels below evaluated level


(Assume a nested sub-document hierarchy with an acl-access control list array - at each level)

{$cond: [{$in: ["UserRole", "acl"] }, "$$DESCEND", "$$PRUNE"]}
as long as UserRole was present in the acl [] array, $$DESCEND will return..once UserRole is not present, $$PRUNE is returned, and the sub-document is redacted

$out
https://docs.mongodb.com/manual/reference/operator/aggregation/out/

{$out: <outputCollection>}
$out must be the last stage in a pipeline

$out will create a new collection or overwrite an existing collection
honors indexes on existing collection


$merge
https://docs.mongodb.com/manual/reference/operator/aggregation/merge/

$merge: a more robust version of $out.  last stage in an aggregation pipeline.

{$merge: {
  into: <target>,
  whenNotMatched: "insert",
  whenMatched: "merge"
}}

{$merge: {
  into: <target>,
  whenMatched: [{$addFields: {total: {$sum: ["$total", "$$new.total"]}}}]
}}

{$merge: {
  into: <target>,
  whenMatched: [{$set: {total: {$sum: ["$total", "$$new.total"]}}}]
}}

USE CASE: aggregate temp data to another collection

db.temp.aggregate([
{...} // pipeline to massage and cleanse data in temp
{$merge: {
  into: "dataCollection",
  whenMatched: "fail"
}}])


###
### VIEWS IN MONGODB
###
(system.view collection)

https://docs.mongodb.com/manual/reference/method/db.createView/

non-materialized views .. a way to use an aggregation pipeline as a "collection"

Vertical slicing: through a $project stage (transform the document shape being returned. limit fields)

db.customers.aggregate([{$project: {_id: 0, accountType: 1}}])

Horizontal slicing: through a $match stage (limit the NUMBER OF DOCUMENTS being returned)

db.customers.aggregate([{$match: {accountType: "bronze"}}, {$project: {_id: 0, accountType: 1}}])

db.createView("viewName", "sourceCollection", [{aggregation}, {pipeline}])



###
### AGGREGATION PERFORMANCE
###

"Realtime" processing vs Batch processing

realtime: provide data for applications. query performance more important.

Batch: provide data for analytics, performance is less important

INDEXES should be used by aggregation queries.
  -once a stage encounters documents that can't use indexes, subsequent stages cannot use indexes
  
{explain: true} option document... USE TO DETERMINE HOW AGG QUERIES RUN AND IF INDEXES ARE BEING USED

$match and $sort, which can use indexes, SHOULD BE AT THE FRONT OF AGGREGATION PIPELINES

transformations in an agg pipeline can hamper ability to use indexes

$limit and $sort should be near each other, and close to front.
  -server can do a "top-k sorting algorithm"
  
QUERY OPTIMIZER RUNS IN BACKGROUND


###
### MEMORY CONSTRAINTS
###

16mb document size limit
transforms can violate this, but whatever is returned, must be within 16mb.

$limit and $project can reduce document size overheard.

100MB RAM limit per stage ({allowDiskUse: true} can mitigate this.  but is not ideal.  LAST RESORT!)

LARGEST STAGES MUST USE INDEXES!


###
### AGGREGATION PIPELINE ON A SHARDED CLUSTER
###

standalone and replica sets ... all data is in the same place.

sharded cluster, data is separated into shards

###
### PIPELINE OPTIMIZATION
###

"covered query": no fetch stage - because it just uses info from INDEX

AVOID NEEDLESS $PROJECTS!  THE AGGREGATION FRAMEWORK WILL PROJECT AWAY UNNECESSARY FIELDS FOR US
-eg, $match then $group on a single field....don't need to only $project that field, the optimizer will do the work for us.

USE THESE BEFORE AN $UNWIND, IF POSSIBLE:
$map
$reduce
$filter
-accumulator expressions available on $project stage

$reduce can implement any high order array function.



